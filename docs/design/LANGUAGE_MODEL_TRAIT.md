# LanguageModel Trait Design

## Problem

Currently, 34+ models each implement their own `forward()` method with:
1. **Inconsistent signatures** - some accept `request_id`, some generate it
2. **Duplicated PP logic** - hook calls copy-pasted across models
3. **Bug-prone** - models like `quantized_mistral3` generate new UUIDs per forward, breaking PP

## Solution

Extend the existing `LanguageModel` trait to handle PP orchestration generically.

## Design

### 1. ForwardContext - Pipeline Parallelism State

```rust
/// Context for pipeline parallelism coordination.
/// Passed from Pipeline to Model, NOT generated by model.
pub struct ForwardContext {
    /// Request correlation ID (from engine, NOT generated by model)
    pub request_id: Uuid,
    /// Token IDs for this forward pass
    pub tokens: Vec<u32>,
    /// Sequence position for KV cache
    pub sequence_position: usize,
    /// Whether this is prefill or decode
    pub is_prefill: bool,
    /// Total prompt tokens (for PP init)
    pub total_prompt_tokens: usize,
}
```

### 2. Extended LanguageModel Trait

```rust
pub trait LanguageModel: Send + Sync {
    /// Model-specific state (positions, paged attention, etc.)
    type State;

    // === Core operations (model-specific) ===

    /// Convert token IDs to embeddings [batch, seq] -> [batch, seq, hidden]
    fn embed(&self, input_ids: &Tensor) -> Result<Tensor>;

    /// Forward through transformer layers (mutates KV cache)
    fn forward_layers(&self, hidden: Tensor, input_ids: &Tensor, state: &Self::State) -> Result<Tensor>;

    /// Project hidden states to vocabulary logits
    fn lm_head(&self, hidden: Tensor) -> Result<Tensor>;

    // === Stage info (model knows its layer range) ===

    fn is_first_stage(&self) -> bool;
    fn is_last_stage(&self) -> bool;
    fn total_layers(&self) -> usize;

    // === Hook management ===

    fn get_hook(&self) -> Option<&HookContainer>;
    fn set_hook(&mut self, hook: HookContainer);

    // === Orchestrated forward (default impl handles PP) ===

    /// Forward with pipeline parallelism support.
    ///
    /// Default implementation handles all PP coordination:
    /// - First stage: embed() or receive from previous stage
    /// - Forward through layers with hook callbacks
    /// - Last stage: lm_head() or send to next stage
    ///
    /// Models just implement embed/forward_layers/lm_head.
    fn forward_with_pp(
        &self,
        input_ids: &Tensor,
        state: &Self::State,
        ctx: &ForwardContext,
    ) -> Result<Tensor> {
        let hook = self.get_hook();

        // === INIT: Call init_pipeline_request on first prefill chunk ===
        if ctx.is_prefill {
            if let Some(h) = hook {
                h.call_init_pipeline_request(ctx.request_id, ctx.total_prompt_tokens);
            }
        }

        // === STAGE 1: Get initial hidden states ===
        let hidden = if self.is_first_stage() {
            self.embed(input_ids)?
        } else {
            // Middle/last stage: receive from previous stage
            hook.ok_or_else(|| Error::Msg("Non-first stage requires hook".into()))?
                .receive_stage_input()?
        };

        // === STAGE 2: Forward through transformer layers ===
        // Hook callbacks happen inside forward_layers() via macros
        let hidden = self.forward_layers(hidden, input_ids, state)?;

        // === STAGE 3: Produce output ===
        if self.is_last_stage() {
            self.lm_head(hidden)
        } else {
            // Non-last stage: send to next stage
            let h = hook.ok_or_else(|| Error::Msg("Non-last stage requires hook".into()))?;
            h.send_stage_output(&hidden, &ctx.tokens, ctx.request_id, ctx.sequence_position)?;

            if h.needs_external_logits() {
                // First stage in multi-stage: wait for logits from last stage
                h.receive_response_logits()
            } else {
                // Middle stage: return placeholder (not used)
                Tensor::zeros(&[1], DType::F32, &hidden.device()?)
            }
        }
    }
}
```

### 3. Model Implementation (quantized_mistral3 example)

```rust
impl LanguageModel for ModelWeights {
    type State = Mistral3State;

    fn embed(&self, input_ids: &Tensor) -> Result<Tensor> {
        self.tok_embeddings.forward(input_ids)
    }

    fn forward_layers(&self, mut hidden: Tensor, input_ids: &Tensor, state: &Self::State) -> Result<Tensor> {
        let cache = &mut self.cache.normal().0;
        let mask = /* ... */;

        for (i, layer) in self.layers.iter().enumerate() {
            // Hook callbacks still happen here via macros, but request_id
            // comes from the hook's stored context, not generated here
            hidden = layer.forward(&hidden, &mask, &state.seqlen_offsets, cache, state.metadata)?;
        }

        self.norm.forward(&hidden)
    }

    fn lm_head(&self, hidden: Tensor) -> Result<Tensor> {
        MatMul.qmethod_matmul(&hidden.contiguous()?, &*self.output)
    }

    fn is_first_stage(&self) -> bool { self.layer_start == 0 }
    fn is_last_stage(&self) -> bool { self.layer_end >= self.total_layers }
    fn total_layers(&self) -> usize { self.total_layers }

    fn get_hook(&self) -> Option<&HookContainer> { self.hook.as_ref() }
    fn set_hook(&mut self, hook: HookContainer) { self.hook = Some(hook); }
}
```

### 4. Pipeline Usage (gguf.rs)

```rust
// In forward_inputs():
let ctx = ForwardContext {
    request_id,  // From ModelInputs, NOT generated
    tokens: input_ids.flatten_all()?.to_vec1()?,
    sequence_position: seqlen_offsets[0],
    is_prefill: matches!(inference_step, InferenceStep::Prefill { .. }),
    total_prompt_tokens: match inference_step {
        InferenceStep::Prefill { total_prompt_tokens, .. } => total_prompt_tokens,
        _ => 0,
    },
};

let state = Mistral3State {
    seqlen_offsets,
    context_lens,
    paged_attn_meta,
};

// Single dispatch - trait handles PP orchestration
let logits = model.forward_with_pp(&input_ids, &state, &ctx)?;
```

## Benefits

1. **Single source of truth** - PP logic in trait default impl, not duplicated
2. **Correct by construction** - request_id flows from pipeline, models can't generate wrong IDs
3. **Incremental migration** - models can be migrated one at a time
4. **Testable** - each method (embed, forward_layers, lm_head) testable in isolation

## Migration Plan

1. Add `ForwardContext` and extend `LanguageModel` trait
2. Migrate `quantized_mistral3` as proof of concept
3. Update `gguf.rs` to use trait-based dispatch for migrated models
4. Gradually migrate other models (can coexist with old approach)

## Open Questions

1. Should `forward_layers()` still call hook macros internally, or should hooks move entirely to trait?
2. How to handle XLora models that need different forward signature?
3. Should we use `dyn LanguageModel` or keep the enum dispatch?
